random.order = F,
rot.pet = .1, scale = c(4, 0.3), colors = pal)
# 1. 기사 검색 url 생성
url <- 'http://search.hani.co.Kr/Search?command=query&keyword=코로나19&sort=d&period=all&media=magazine'
# 5. url에 해당되는 데이터 가져오기
k <- read_html(url, encoding="utf-8")
# 6. dt 태그 안에 있는 a 태그 들의 href 속성의 값 가져오기
k <- k %>% html_nodes("dt") %>% html_nodes("a") %>% html_attr("href")
k
cat(temp, file="temp2.txt", append=TRUE)
# 8. 파일의 모든 내용 가져오기
txt <- readLines("temp2.txt")
# 9. 명사만 추출
useNIADic()
nouns <-extractNoun(txt)
nouns # 세종 사전보다 훨씬 많은? nouns 딕셔너리 사용
# 10. 빈도수 만들기 - 각 단어가 몇번씩 나왔는지
wordcount <- table(unlist(nouns))
wordcount
# 11. 데이터 프레임으로 변환
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
df_word
class(df_word) # data.frame
# 12. 필터링
df_word <- filter(df_word, nchar(Var1) >= 2)
# 13. 워드클라우드 색상 판 생성
pal <- brewer.pal(8, "Dark2")
# 14. 시드 설정
set.seed(1234)
# 15. 워드 클라우드 생성
wordcloud(words = df_word$Var1,
freq = df_word$Freq,
min.freq = 2, max.word=200,
random.order = F,
rot.pet = .1, scale = c(4, 0.3), colors = pal)
# 1. 기사 검색 url 생성
url <- 'http://search.hani.co.Kr/Search?command=query&keyword=코로나19&sort=d&period=all&media=magazine'
# 2. url에 해당되는 데이터 가져오기
a <- read_html(url, encoding="utf-8")
# 3. dt 태그 안에 있는 a 태그 들의 href 속성의 값 가져오기
a <- a %>% html_nodes("dt") %>% html_nodes("a") %>% html_attr("href")
a
cat(temp2, file="temp2.txt", append=TRUE)
cat(temp, file="temp2.txt", append=TRUE)
for(addr in a){
temp <- read_html(addr) %>% html_nodes(".text") %>% html_text()
cat(temp, file="temp2.txt", append=TRUE)
}
# 8. 파일의 모든 내용 가져오기
txt <- readLines("temp2.txt")
head(txt)
# 9. 명사만 추출
useNIADic()
nouns <-extractNoun(txt)
nouns # 세종 사전보다 훨씬 많은? nouns 딕셔너리 사용
# 10. 빈도수 만들기 - 각 단어가 몇번씩 나왔는지
wordcount <- table(unlist(nouns))
wordcount
# 11. 데이터 프레임으로 변환
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
df_word
class(df_word) # data.frame
# 1. 기사 검색 url 생성
url <- 'http://news.donga.com/search?p=1&query=코로나19&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1'
# 2. url에 해당되는 데이터 가져오기
b <- read_html(url, encoding="utf-8")
# 3. dt 태그 안에 있는 a 태그 들의 href 속성의 값 가져오기
b <- b %>% html_nodes("dt") %>% html_nodes("a") %>% html_attr("href")
source('C:/Rproject/Class/20200406.R', encoding = 'UTF-8', echo=TRUE)
# 3. dt 태그 안에 있는 a 태그 들의 href 속성의 값 가져오기
b <- b %>% html_nodes("dt") %>% html_nodes("b") %>% html_attr("href")
# 1. 기사 검색 url 생성
url <- 'http://news.donga.com/search?p=1&query=코로나19&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1'
# 2. url에 해당되는 데이터 가져오기
b <- read_html(url, encoding="utf-8")
# 3. dt 태그 안에 있는 a 태그 들의 href 속성의 값 가져오기
b <- b %>% html_nodes("dt") %>% html_nodes("b") %>% html_attr("href")
b
temp <- read_html(addr) %>% html_nodes(".text") %>% html_text()
# 5. 파일의 모든 내용 가져오기
txt <- readLines("temp2.txt")
head(txt)
cat(temp, file="temp3.txt", append=TRUE)
# 5. 파일의 모든 내용 가져오기
txt <- readLines("temp3.txt")
head(txt)
# 1. 기사 검색 url 생성
url <- 'http://www.donga.com/news/search?p=16&query=%EC%BD%94%EB%A1%9C%EB%82%9819&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1'
# 2. url에 해당되는 데이터 가져오기
b <- read_html(url, encoding="utf-8")
# 3. dt 태그 안에 있는 a 태그 들의 href 속성의 값 가져오기
b <- b %>% html_nodes("dt") %>% html_nodes("b") %>% html_attr("href")
b
# 4. k에 저장된 모든 URL에 해당하는 데이터의 클래스가 text인 데이터를 읽어서 파일에 저장
# for(임시변수 in 컬렉션이름){} ★★★★★
for(addr in b){
temp <- read_html(addr) %>% html_nodes(".text") %>% html_text()
cat(temp, file="temp3.txt", append=TRUE)
}
# 5. 파일의 모든 내용 가져오기
txt <- readLines("temp3.txt")
# 6. 명사만 추출
useNIADic()
nouns <-extractNoun(txt)
nouns # 세종 사전보다 훨씬 많은? nouns 딕셔너리 사용
# 7. 빈도수 만들기 - 각 단어가 몇번씩 나왔는지
wordcount <- table(unlist(nouns))
wordcount
# 8. 데이터 프레임으로 변환
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
df_word
class(df_word) # data.frame
# 9. 필터링
df_word <- filter(df_word, nchar(Var1) >= 2)
# 10. 워드클라우드 색상 판 생성
pal <- brewer.pal(8, "Dark2")
# 11. 시드 설정
set.seed(1234)
# 12. 워드 클라우드 생성
wordcloud(words = df_word$Var1,
freq = df_word$Freq,
min.freq = 2, max.word=200,
random.order = F,
rot.pet = .1, scale = c(4, 0.3), colors = pal)
# 1. 기사 검색 url 생성
url <- 'http://search.hani.co.kr/Search?command=query&keyword=%EC%BD%94%EB%A1%9C%EB%82%98&media=news&submedia=&sort=d&period=all&datefrom=2000.01.01&dateto=2020.04.06&pageseq=4'
# 2. url에 해당되는 데이터 가져오기
a <- read_html(url, encoding="utf-8")
# 3. dt 태그 안에 있는 a 태그 들의 href 속성의 값 가져오기
a <- a %>% html_nodes("dt") %>% html_nodes("a") %>% html_attr("href")
# 4. k에 저장된 모든 URL에 해당하는 데이터의 클래스가 text인 데이터를 읽어서 파일에 저장
# for(임시변수 in 컬렉션이름){} ★★★★★
for(addr in a){
temp <- read_html(addr) %>% html_nodes(".text") %>% html_text()
cat(temp, file="temp2.txt", append=TRUE)
}
# 5. 파일의 모든 내용 가져오기
txt <- readLines("temp2.txt")
head(txt)
# 6. 명사만 추출
useNIADic()
nouns <-extractNoun(txt)
nouns # 세종 사전보다 훨씬 많은? nouns 딕셔너리 사용
# 7. 빈도수 만들기 - 각 단어가 몇번씩 나왔는지
wordcount <- table(unlist(nouns))
wordcount
# 8. 데이터 프레임으로 변환
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
df_word
class(df_word) # data.frame
# 9. 필터링
df_word <- filter(df_word, nchar(Var1) >= 2)
# 10. 워드클라우드 색상 판 생성
pal <- brewer.pal(8, "Dark2")
# 11. 시드 설정
set.seed(1234)
# 12. 워드 클라우드 생성
wordcloud(words = df_word$Var1,
freq = df_word$Freq,
min.freq = 2, max.word=200,
random.order = F,
rot.pet = .1, scale = c(4, 0.3), colors = pal)
# 1. 기사 검색 url 생성
url <- 'http://www.donga.com/news/search?p=16&query=%EC%BD%94%EB%A1%9C%EB%82%9819&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1'
# 2. url에 해당되는 데이터 가져오기
b <- read_html(url, encoding="utf-8")
# 3. dt 태그 안에 있는 a 태그 들의 href 속성의 값 가져오기
b <- b %>% html_nodes("dt") %>% html_nodes("b") %>% html_attr("href")
b
# 4. k에 저장된 모든 URL에 해당하는 데이터의 클래스가 text인 데이터를 읽어서 파일에 저장
# for(임시변수 in 컬렉션이름){} ★★★★★
for(addr in b){
temp <- read_html(addr) %>% html_nodes(".text") %>% html_text()
cat(temp, file="temp3.txt", append=TRUE)
}
# 5. 파일의 모든 내용 가져오기
txt <- readLines("temp3.txt")
head(txt)
# 6. 명사만 추출
useNIADic()
nouns <-extractNoun(txt)
nouns # 세종 사전보다 훨씬 많은? nouns 딕셔너리 사용
# 7. 빈도수 만들기 - 각 단어가 몇번씩 나왔는지
wordcount <- table(unlist(nouns))
wordcount
# 8. 데이터 프레임으로 변환
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
df_word
class(df_word) # data.frame
# 9. 필터링
df_word <- filter(df_word, nchar(Var1) >= 2)
# 10. 워드클라우드 색상 판 생성
pal <- brewer.pal(8, "Dark2")
# 11. 시드 설정
set.seed(1234)
# 12. 워드 클라우드 생성
wordcloud(words = df_word$Var1,
freq = df_word$Freq,
min.freq = 2, max.word=200,
random.order = F,
rot.pet = .1, scale = c(4, 0.3), colors = pal)
# 12. 워드 클라우드 생성
wordcloud2(words = df_word$Var1,
freq = df_word$Freq,
min.freq = 2, max.word=200,
random.order = F,
rot.pet = .1, scale = c(4, 0.3), colors = pal)
# 12. 워드 클라우드 생성
wordcloud(words = df_word$Var1,
freq = df_word$Freq,
min.freq = 2, max.word=200,
random.order = F,
rot.pet = .1, scale = c(4, 0.3), colors = pal)
# 12. 워드 클라우드 생성
wordcloud(words = df_word$Var1,
freq = df_word$Freq,
min.freq = 2, max.word=200,
random.order = F,
rot.pet = .1, scale = c(4, 0.3), colors = pal, background=black)
# 12. 워드 클라우드 생성
wordcloud(words = df_word$Var1,
freq = df_word$Freq,
min.freq = 2, max.word=200,
random.order = F,
rot.pet = .1, scale = c(4, 0.3), colors = pal)
# 1. 기사 검색 url 생성
url <- 'http://search.hani.co.kr/Search?command=query&keyword=%EC%BD%94%EB%A1%9C%EB%82%98&media=news&submedia=&sort=d&period=all&datefrom=2000.01.01&dateto=2020.04.06&pageseq=4'
# 2. url에 해당되는 데이터 가져오기
a <- read_html(url, encoding="utf-8")
# 3. dt 태그 안에 있는 a 태그 들의 href 속성의 값 가져오기
a <- a %>% html_nodes("div") %>% html_nodes("a") %>% html_attr("href")
a
# 4. k에 저장된 모든 URL에 해당하는 데이터의 클래스가 text인 데이터를 읽어서 파일에 저장
# for(임시변수 in 컬렉션이름){} ★★★★★
for(addr in a){
temp <- read_html(addr) %>% html_nodes(".text") %>% html_text()
cat(temp, file="temp2.txt", append=TRUE)
}
# 1. 기사 검색 url 생성
url <- 'http://www.donga.com/news/search?p=16&query=%EC%BD%94%EB%A1%9C%EB%82%9819&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1'
# 2. url에 해당되는 데이터 가져오기
b <- read_html(url, encoding="utf-8")
# 3. dt 태그 안에 있는 a 태그 들의 href 속성의 값 가져오기
b <- b %>% html_nodes("div") %>% html_nodes("a") %>% html_attr("href")
b
# 4. k에 저장된 모든 URL에 해당하는 데이터의 클래스가 text인 데이터를 읽어서 파일에 저장
# for(임시변수 in 컬렉션이름){} ★★★★★
for(addr in b){
temp <- read_html(addr) %>% html_nodes(".text") %>% html_text()
cat(temp, file="temp3.txt", append=TRUE)
}
# 5. 파일의 모든 내용 가져오기
txt <- readLines("temp3.txt")
head(txt)
# 6. 명사만 추출
useNIADic()
nouns <-extractNoun(txt)
nouns # 세종 사전보다 훨씬 많은? nouns 딕셔너리 사용
# 7. 빈도수 만들기 - 각 단어가 몇번씩 나왔는지
wordcount <- table(unlist(nouns))
wordcount
# 8. 데이터 프레임으로 변환
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
df_word
class(df_word) # data.frame
# 9. 필터링
df_word <- filter(df_word, nchar(Var1) >= 2)
# 10. 워드클라우드 색상 판 생성
pal <- brewer.pal(8, "Dark2")
# 11. 시드 설정
set.seed(1234)
# 12. 워드 클라우드 생성
wordcloud(words = df_word$Var1,
freq = df_word$Freq,
min.freq = 2, max.word=200,
random.order = F,
rot.pet = .1, scale = c(4, 0.3), colors = pal)
# 1. 기사 검색 url 생성
url <- 'http://www.donga.com/news/search?p=16&query=%EC%BD%94%EB%A1%9C%EB%82%9819&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1'
# 2. url에 해당되는 데이터 가져오기
b <- read_html(url, encoding="utf-8")
# 3. dt 태그 안에 있는 a 태그 들의 href 속성의 값 가져오기
b <- b %>% html_nodes("div") %>% html_nodes("p") %>% html_nodes("a") %>% html_attr("href")
b
# 4. k에 저장된 모든 URL에 해당하는 데이터의 클래스가 text인 데이터를 읽어서 파일에 저장
# for(임시변수 in 컬렉션이름){} ★★★★★
for(addr in b){
temp <- read_html(addr) %>% html_nodes(".text") %>% html_text()
cat(temp, file="temp3.txt", append=TRUE)
}
# 5. 파일의 모든 내용 가져오기
txt <- readLines("temp3.txt")
head(txt)
# 1. 기사 검색 url 생성
url <- 'http://www.donga.com/news/search?p=16&query=%EC%BD%94%EB%A1%9C%EB%82%9819&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1'
# 2. url에 해당되는 데이터 가져오기
b <- read_html(url, encoding="utf-8")
# 3. dt 태그 안에 있는 a 태그 들의 href 속성의 값 가져오기
b <- b %>% html_nodes("div") %>% html_nodes("p.tit") %>% html_nodes("a") %>% html_attr("href")
b
# 4. k에 저장된 모든 URL에 해당하는 데이터의 클래스가 text인 데이터를 읽어서 파일에 저장
# for(임시변수 in 컬렉션이름){} ★★★★★
for(addr in b){
temp <- read_html(addr) %>% html_nodes(".text") %>% html_text()
cat(temp, file="temp3.txt", append=TRUE)
}
# 5. 파일의 모든 내용 가져오기
txt <- readLines("temp3.txt")
head(txt)
# 6. 명사만 추출
useNIADic()
nouns <-extractNoun(txt)
nouns # 세종 사전보다 훨씬 많은? nouns 딕셔너리 사용
# 7. 빈도수 만들기 - 각 단어가 몇번씩 나왔는지
wordcount <- table(unlist(nouns))
wordcount
# 8. 데이터 프레임으로 변환
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
df_word
class(df_word) # data.frame
# 9. 필터링
df_word <- filter(df_word, nchar(Var1) >= 2)
# 10. 워드클라우드 색상 판 생성
pal <- brewer.pal(8, "Dark2")
# 11. 시드 설정
set.seed(1234)
# 12. 워드 클라우드 생성
wordcloud(words = df_word$Var1,
freq = df_word$Freq,
min.freq = 2, max.word=200,
random.order = F,
rot.pet = .1, scale = c(4, 0.3), colors = pal)
# 1. 기사 검색 url 생성
url <- 'http://www.donga.com/news/search?p=16&query=%EC%BD%94%EB%A1%9C%EB%82%9819&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1'
# 2. url에 해당되는 데이터 가져오기
b <- read_html(url, encoding="utf-8")
# 3. dt 태그 안에 있는 a 태그 들의 href 속성의 값 가져오기
b <- b %>% html_nodes("div") %>% html_nodes("p.tit") %>% html_nodes("a") %>% html_attr("href")
b
# 4. k에 저장된 모든 URL에 해당하는 데이터의 클래스가 text인 데이터를 읽어서 파일에 저장
# for(임시변수 in 컬렉션이름){} ★★★★★
for(addr in b){
temp <- read_html(addr) %>% html_nodes(".text") %>% html_text()
cat(temp, file="temp_corona.txt", append=TRUE)
}
# 5. 파일의 모든 내용 가져오기
txt <- readLines("temp_corona.txt")
head(txt)
# 6. 명사만 추출
useNIADic()
nouns <-extractNoun(txt)
nouns # 세종 사전보다 훨씬 많은? nouns 딕셔너리 사용
# 7. 빈도수 만들기 - 각 단어가 몇번씩 나왔는지
wordcount <- table(unlist(nouns))
wordcount
# 8. 데이터 프레임으로 변환
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
df_word
class(df_word) # data.frame
# 9. 필터링
df_word <- filter(df_word, nchar(Var1) >= 2)
# 10. 워드클라우드 색상 판 생성
pal <- brewer.pal(8, "Dark2")
# 11. 시드 설정
set.seed(1234)
# 12. 워드 클라우드 생성
wordcloud(words = df_word$Var1,
freq = df_word$Freq,
min.freq = 2, max.word=200,
random.order = F,
rot.pet = .1, scale = c(4, 0.3), colors = pal)
# 12. 워드 클라우드 생성
wordcloud(words = df_word$Var1,
freq = df_word$Freq,
min.freq = 2, max.word=100,
random.order = F,
rot.pet = .1, scale = c(4, 0.3), colors = pal)
# 1. 기사 검색 url 생성
url <- 'http://www.donga.com/news/search?p=16&query=%EC%BD%94%EB%A1%9C%EB%82%9819&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1'
# 2. url에 해당되는 데이터 가져오기
b <- read_html(url, encoding="utf-8")
# 3. dt 태그 안에 있는 a 태그 들의 href 속성의 값 가져오기
b <- b %>% html_nodes(html, '.searchCont') %>% html_nodes("a") %>% html_attr("href")
# 3. dt 태그 안에 있는 a 태그 들의 href 속성의 값 가져오기
b <- b %>% html_nodes(html, '.searchCont') %>% html_nodes("p.tit") %>% html_nodes("a") %>% html_attr("href")
# 1. 기사 검색 url 생성
url <- 'http://www.donga.com/news/search?p=16&query=%EC%BD%94%EB%A1%9C%EB%82%9819&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1'
# 2. url에 해당되는 데이터 가져오기
b <- read_html(url[1], encoding="utf-8")
# 3. dt 태그 안에 있는 a 태그 들의 href 속성의 값 가져오기
# b <- b %>% html_nodes(html, '.searchCont') %>% html_nodes("p.tit") %>% html_nodes("a") %>% html_attr("href")
# b
html_nodes(html, '.searchCont')
# 1. 기사 검색 url 생성
url <- 'http://www.donga.com/news/search?p=16&query=%EC%BD%94%EB%A1%9C%EB%82%9819&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1'
# 1. 기사 검색 url 생성
url <- 'http://www.donga.com/news/search?p=16&query=%EC%BD%94%EB%A1%9C%EB%82%9819&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1'
# 2. url에 해당되는 데이터 가져오기
read_html(urls[1])
# 2. url에 해당되는 데이터 가져오기
read_html(urls[1])
# 2. url에 해당되는 데이터 가져오기
read_html(url[1])
# 3. dt 태그 안에 있는 a 태그 들의 href 속성의 값 가져오기
# b <- b %>% html_nodes(html, '.searchCont') %>% html_nodes("p.tit") %>% html_nodes("a") %>% html_attr("href")
# b
html_nodes(html, '.searchCont')
# 3. dt 태그 안에 있는 a 태그 들의 href 속성의 값 가져오기
# b <- b %>% html_nodes(html, '.searchCont') %>% html_nodes("p.tit") %>% html_nodes("a") %>% html_attr("href")
# b
html2 <- html_nodes(html, '.searchCont')
# 2. url에 해당되는 데이터 가져오기
read_html(url[1])
# 3. dt 태그 안에 있는 a 태그 들의 href 속성의 값 가져오기
# b <- b %>% html_nodes(html, '.searchCont') %>% html_nodes("p.tit") %>% html_nodes("a") %>% html_attr("href")
# b
html2 <- html_nodes(html, '.searchCont')
# 3. dt 태그 안에 있는 a 태그 들의 href 속성의 값 가져오기
# b <- b %>% html_nodes(html, '.searchCont') %>% html_nodes("p.tit") %>% html_nodes("a") %>% html_attr("href")
# b
html2 <- html_nodes(html, '.searchCont')
# 1. 기사 검색 url 생성
basic_url <- 'http://www.donga.com/news/search?p=16&query=%EC%BD%94%EB%A1%9C%EB%82%9819&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1'
# 2. url에 해당되는 데이터 가져오기
urls <- NULL
links <- NULL
links <- links[-grep("pdf", links)]
txts <- NULL
head(txts)
# 1. 기사 검색 url 생성
url <- 'http://www.donga.com/news/search?p=16&query=%EC%BD%94%EB%A1%9C%EB%82%9819&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1'
# 2. url에 해당되는 데이터 가져오기
b <- read_html(url, encoding="utf-8")
# 3. dt 태그 안에 있는 a 태그 들의 href 속성의 값 가져오기
b <- b %>% html_nodes(html, '.searchCont') %>% html_nodes('a') %>% html_attr("href") %>% unique()
b
# 3. dt 태그 안에 있는 a 태그 들의 href 속성의 값 가져오기
b <- b %>% html_nodes(html, '.searchCont') %>% html_nodes('a') %>% html_attr("href")
b
# 1. 기사 검색하기
url <- "http://news.donga.com/search?p=1&query=코로나19&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1"
url
link <- read_html(url)
link
# 2. 기사 링크 가져오기
addr <- link %>% html_nodes(".t") %>% html_nodes(".tit") %>% html_nodes("a") %>% html_attr("href")
addr
# 3. 링크를 출력하기
for(url in addr){
print(url)  #url에 있는 기사들을 모아서 파일에 저장
}
# 4. addr의 모든 내용을 순회하면서
# 기사 내용을 article.txt에 저장
for(url in addr){
# url에 있는 기사들을 모아서 파일에 저장
# article_txt 클래스에 있는 내용만 저장하기
content = read_html(url) %>% html_nodes(".article_txt") %>% html_text()
# print(content)
# 저장하기
cat(content,file="article.txt", append = T)
}
print(content)
# 5. 파일의 모든 내용 가져오기
txt <- readLines("article.txt")
head(txt)
# 6. 명사만 추출
useNIADic()
nouns <- extractNoun(txt)
nouns
# 7. 빈도수 만들기 - 각 단어가 몇번씩 나왔는지
wordcount <- table(unlist(nouns))
wordcount
# 8. 데이터 프레임으로 변환
df_word <- as.data.frame(wordcount, stringsAsFactors = F)
df_word
class(df_word)
# 9. 필터링
df_word <- filter(df_word, nchar(Var1) >= 2)
# 10. 워드클라우드 색상 판 생성
pal <- brewer.pal(8, "Dark2")
# 11. 시드 설정
set.seed(1234)
# 12. 워드 클라우드 생성
wordcloud(words = df_word$Var1,
freq=df_word$Freq,
min.freq = 2, max.words=200,
random.order=F,
rot.per=.1, scale=c(4, 0.3), colors=pal)
#ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ
# R 키보드 이용한 데이터 입력
# c() 함수를 이용한 데이터 입력
x <- c(10.4, 5, 6, 3.1, 6.4, 21.7)
x
# scan() 함수 이용
x <- scan()
x
# 문자열 입력받고 싶을 때
x <- scan(what=" ")
# 1: Actor
# 2: Actress
# 3: Hero
# 4: Heroine
# 5:
#   Read 4 items
x
age = data.frame()
age = edit(age)
age = data.frame()
age = edit(age)
age
age = edit(age)
useSejongDic()
